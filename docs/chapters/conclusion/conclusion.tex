\chapter{Conclusion and Future Work}
\label{chap:conclusion}
\section{Conclusion}
During this research, we identified that many \faas{} workloads typically accept HTTP requests, construct a query and send it to a database for processing, before performing some form of preprocessing with the result, and returning it in the response.

Whilst awaiting the query result, functions are often idle yet are charged for the allocated but unused resources. From this observation, we propose executing functions in a split manner, whereby the first part of a function invokes an asynchronous request, and the second part processes the result. A cloud-provider-agnostic parameterised costing model is defined and then used to model various scenarios to determine the cost effectiveness of splitting functions.

Following this, we model response times of asynchronous requests using a Weibull distribution, and use this to determine the probability of a function split resulting in a cost reduction. We then use this probability to determine whether to split a function or retain the allocated resources and wait for the request to complete.

We then developed \faaasc{}, a system that automatically analyses and generates split \js{} \faas{} functions with minimal developer input, and evaluated the overheads associated with executing typical functions using different splitting strategies on AWS Lambda. Deploying to AWS, we developed a gateway, and a proxy around PostgreSQL that allows queries to invoke continuation functions passed to them once a query result is available.

We found that for OLAP workloads, split functions provide a 75\% cost reduction over executing the same function without splitting. We also found that for OLTP workloads, the overhead and penalty of splitting functions outweighs the cost reduction with the current AWS Lambda billing model.

We also introduce an adaptive splitting strategy, which deploys a monitor alongside a \faas{} function to monitor the response times of asynchronous requests and infers a response time distribution, which is used by executing functions to determine whether to defer to a proxy service to perform asynchronous requests, or execute the asynchronous request locally, retaining the allocated resources.

This adaptive splitting strategy was evaluated on AWS Lambda, and found to select the optimal splitting strategy, albeit with a short time-delay for the empirical response-time data to be aggregated and the response time distribution to be correctly inferred.

Finally, we consider a set of hypothetical scenarios with adjusted \faas{} billing models, and model how \faaas{} would perform under these scenarios.

\section{Future Work}
\label{sec:future-work}
In this section, we will discuss future work and directions that could be taken to extend the work presented in this thesis.

\subsection{Extending \faaasc{}}
A major unaddressed bottleneck of \faaas{} currently relates to the overhead of serialising and deserialising context state passed between function invocations. Whilst this is necessary in order to persist state between function invocation and handler, in the pathalogical case where a variable is declared at the start of the program, and only used at the end, the variable will need to be serialised and deserialised through each continuation.

This could be addressed in a variety of different ways, for eaxmple, a DAG of the JS program could be generated and the serialisation and deserialisation could be optimised to only include the variables that are used directly in the next continuation. Alternatively, all context could be written to a low latency cache such as Redis, and lazily loaded by continuations when they need it. This would mean that only a pointer to this data would need to be serialised and deserialised, rather than the data itself, however it would introduce additional network round-trips whenever data is accessed.

Additionaly, another bottlneck of the system is the latency whilst fetching the response time distribution parameters from the Redis cache in order to determine whether to locally invoke the asynchornous operation or delegate it to a proxy. This could be addressed by making a request directly to the proxy, which would internally determine an estimate for the asynchronous request. For instance, the database proxy could use the query estimate used by the plan optimiser to compute the probability that the function would be profitable if it split. In the case that the function should split, the proxy could then return the queue to serialise the context and the continuation to before exiting, otherwise if the split is not profitable, the proxy could just execute the query and directly return the result. This would reduce the latency by the round-trip time to the Redis cache.

\subsection{Potential solutions that would make OLTP workloads more viable}
In this section, we discuss potential solutions which would make OLTP workloads more viable on \faas{} platforms.

\subsubsection{Voluntary release of CPU time back to the hypervisor}
As discussed in Section TODO\todo{reference to background where cloud providers typically use vms to isolate functions}, cloud providers typically use virtual machines to isolate functions. The hypervisor is responsible for scheduling CPU time to each of the virtual machines and does so fairly, based on the resource allocation of the function to be executed.

From the results of the decribed experiments, it is clear that for the proportion of the time the function is executing, functions are allocated CPU resources which they are not using. The ability to voluntarily release this CPU time back to the hypervisor would allow the hypervisor to allocate this CPU time to other functions, and thus reduce the cost of executing the function, since only the memory must be allocated for the entire duration of the function's execution.

\subsubsection{Voluntary release of memory back to the hypervisor}
Whilst releasing CPU allocation back to the hypervisor when blocked on IO could reduce the cost of executing the function, memory would still need to be billed since it occupies memory that another function could be using. In the case that a function is blocked on IO, depending on the disk latency, it could be more cost effective to write the blocked function's memory to swap during the period that it is blocked, and then load the function back into memory once the response has arrived, and the node has capacity to continue execution of the function.

This would have the impact of increasing latency, as the function would need to perform a write-read cycle to and from disk, in addition to waiting until the hypervisor has capacity to load the function back into memory. This would however allow functions to be billed more accurately based on CPU time and memory that they have utilised.

We propose a short experiment in order to assess the viability of this in AWS Lambda. By inspecting the CloudWatch logs from executing the experiment benchmarks, we observe that the average memory consumption for all the NodeJS functions was approximately \SI{108}{\mega\byte}. Since we cannot know the specifics of the disk used by the instances running the hypervisor to execute Lambda functions, we will assume that the write speed to the EFS (Elastic File System) from the Lambda environment is representative of the disk speed of the hypervisor. We deploy a simple function that writes and then reads a \SI{108}{\mega\byte} file of random data to and from EFS, and measure the time taken to perform this operation. This file is to be representative of uncompressed memory allocated to the MicroVM that is to be written to swap during a blocked IO operation. The results of this are shown in Figure \todo{run EFS IO benchmark}.

In order to achieve this technically, the operating system could notify the hypervisor that the function is blocked on IO when the operation system recieves an \verb|epoll\_wait| system call. The main difficulty with this approach would likely be the issue of correctly predicting when a call to \verb|epoll\_wait| would cause the function to yield for a long enough duration for it to be worth the hypervisor to swap the function out to disk.

Therefore, it may be more effective to implement support for this at a language level, whereby the language runtime would notify the hypervisor that the function is blocked on IO, and that the hypervisor should swap the function out to disk. If this occurs at the language level, then it would have access to context around the asynchronous IO operation, and could use this to predict its duration.

\subsubsection{Function continuation execution within the database}
We have so far focused on potential solutions that would allow for better resource utilisation of the hypervisor. Whilst an important component to the solution, it does not address the issues of latency encountered by performing the AWS Lambda slotting algorithm for each continuation. By formalising serverless functions as handlers which return continuations in Section \todo{reference back to continuations}, we could pass the continuation directly to the database to execute after the query has been completed. This however still relies on low invocation costs in order to be cost effective.

Mainstream cloud providers typically use either container based isolation or VM based isolation, which both have relatively high invocation overhead. Other isolation mechanisms, such as the WASM isolation methods described in Section \todo{Refernce back to WASM isolation section in background}, have the potential to reduce invocation overhead, allowing continuations to be executed with a much lower cost penalty.

By embedding a WASM runtime into the database, it could enable execution of the continuations reutrned by \faaasc{} handlers directly inside the database.
