\chapter{Background}

\section{Terminology}
\subsection{What is serverless?}
\todo[inline]{Cite page 168 of wardley maps book}
Serverless computing emerged in the mid-to-late 2000s\cite{wardleyWardleyMaps2022,IntroducingGoogleApp,patilServerlessComputingEmergence2021} as a new paradigm for deploying applications, with the emergence of low-cost public cloud providers\cite{patilServerlessComputingEmergence2021,BenjaminBlackEC2}. It allows developers to deploy applications without managing the underlying infrastructure, leading to much more scalable and cost-effective solutions.

Typically, resources such as VMs\cite{hoeferTaxonomyCloudComputing2010} are rented in sub-second increments, and with storage and network charged by total usage. This results in a Pay-as-you-go (PAYG) model that can adapt to highly variable workloads\cite{sehgalCostBillingPractices2023,hilleyCloudComputingTaxonomy2009}.

Serverless applications are usually composed of cloud provider managed services, such as databases, storage, and compute, and are often event-driven\cite{EventarcOverview,EventListenerAmazon,robeceOverviewAzureEvent2024}. This means that application logic can be invoked by events, such as HTTP requests, database changes, or file uploads. This allows the application to scale independently\cite{goniwadaCloudNativeArchitecture2022}, as the cloud provider can provision resources as needed.

\subsection{What is a VM?}
Virtual Machines (VMs) are a form of virtualisation that enables multiple operating systems to run on a single physical host machine\cite{ramosjoaocarloscarvalhodossantosSecurityChallengesVirtualization2009}. They run ontop of a hypervisor or virtual machine monitor (VMM), and are typically used to run systems for multiple tennants on the same physical hardware. Typically VMs enable more optimal use of hardware resources \cite{desaiHypervisorSurveyConcepts2013}.

Since VMs do not share a kernel, they provide a high level of isolation between VMs\cite{hoeferTaxonomyCloudComputing2010}, whereby isolation is enforced by the VMM. This eliminates an entire set of security vulnerabilities, notabily kernel layer vulnerabilities, since for a malicious VM to access another, it must find vulnerabilities in the kernel. Whilst VMs reduce the attack surface for vulnerabilities, they do not eliminate it entirely, as there could still be vulnerabilities in the VMMs itself\cite{reubenSurveyVirtualMachine,ramosjoaocarloscarvalhodossantosSecurityChallengesVirtualization2009}.

Additionally, hypervisors tend to allocate fixed resources for the lifetime of the VM, allowing for guaranteed CPU time, memory and storage\cite{hoeferTaxonomyCloudComputing2010}, however can utilise ballooning to overprovision memory and borrow/steal this from VMs running on the hypervisor\cite{moniruzzamanAnalysisMemoryBallooning2014}. Additionally, hotplugging\cite{hildenbrandVirtiomemParavirtualizedMemory2021,LKMLDanielKiper} allows for dynamically adding and removing CPU shares (referred to as vCPUs) and memory from a VM without downtime.

VMs tend to have high startup times\cite{haoEmpiricalAnalysisVM2021}, since they must boot an entire operating system, and are much more heavy-weight compared with other virtualisation technologies, since they must run a full kernel and operating system.

\subsection{What is a container?}
Containers were popularised by Docker\cite{DockerAcceleratedContainer2022} in the early 2010s, and have since become the de-facto standard\cite{vanoCloudNativeWorkloadOrchestration2023} for deploying applications to cloud and serverless environments. Containers are a form of operating system level virtualisation\cite{yadavDockerContainersVirtual2019}, whereby the kernel is shared between containers\cite{WhatContainerDockera}, and each container runs in its own userspace. This allows for much more lightweight virtualisation compared with VMs, since containers do not need to boot an entire operating system, and can share the kernel with the host\cite{potdarPerformanceEvaluationDocker2020}.

\subsubsection{CGroups and Namespaces}
Containers are interally just linux processes — the same type of processes that typically would run inside a VM. The core difference however is the use of CGroups and Namespaces\cite{rosenramiNamespacesCgroupsBasis2016} attached to the process that isolate the container from other processes running on the same operating system.

Typically, in order to provide isolation between containers, namespaces are used\cite{NamespacesLinuxManual}. Whilst namespace level security should provide isolation for a container, it cannot provide protection against kernel layer attacks, whereby a malicious container exploits a vulnerability in the kernel to escalate its permissions and take control of the host system\cite{CVECVE202014386}, and in turn other containers\cite{linMeasurementStudyLinux2018}. Kernel level security mechanisms have been shown to provide additional protection against this\cite{sunSecurityNamespaceMaking2018}, in addition to restricting particular vulnerable syscalls from the container to the host\cite{GVisor}.

In order to control and meter resource usage, containers use CGroups, allowing memory usage and CPU time shares to be controlled\cite{CgroupsLinuxManual}. This allows for fine-grained control over resource usage, and can be used to prevent a single container from consuming all resources on a host\cite{ContainerSecurityFundamentals}.

\subsubsection{Container Runtimes vs OCI runtimes}
The container runtime is the core of the container system, and is responsible for creating and running containers throughout its lifecycle, in addition to managing pulled container images on a host\cite{espePerformanceEvaluationContainer2020}. There are a few popular container runtimes, specifically containerd and CRI-O, which are both OCI compliant runtimes. Their role is to manage the construction of an OCI bundle from an OCI image, and delegate execution to the OCI runtime. Containerd and CRI-O both use Runc as their OCI runtime, which is responsible for creating and running containers from OCI bundles created by the container runtime.

Within the container runtime, the OCI runtime is responsible for taking the compiled OCI bundle, creating a container from this, and executing the container. The two main container runtimes are RunC\cite{OpencontainersRunc2024} and gVisor\cite{GVisor}. gVisor typically provides a higher level of isolation compared with RunC, however it suffers slightly from reduced performance\cite{espePerformanceEvaluationContainer2020}.

\subsection{What is \faas{}?}
Derived from the success of serverless computing, \faas{} rose to prominence in the mid-2010s\cite{AmazonWebServices2014,azureAnnouncingGeneralAvailability2016}. It allows developers to write code as functions that are executed in sandboxed environments in response to events, such as HTTP requests, database changes, or file uploads\cite{EventarcOverview,EventListenerAmazon,robeceOverviewAzureEvent2024}. Typically, serverless functions are billed for the duration of their execution in addition to a flat invocation cost, allowing for fine-grained billing of resources\cite{bortoliniInvestigatingPerformanceCost2020}. Additionally, since the underlying infrastructure of the function is abstracted away from the developer, the responsibility for scaling the function is moved to the cloud provider.

\section{\faas{} isolation mechanisms}
Typically, multi-tenany public \faas{} providers can be characterised into two categories: providers offering VM based isolation\cite{agacheFirecrackerLightweightVirtualization2020} and providers offering container based isolation\cite{GVisor}. Both types provide differing levels of security guarantees, and have different performance characteristics.

\subsection{VM based isolation}
VM based isolation is typically considered a much more secure form of isolation between \faas{} functions on multitenant public clouds\cite{jithinVirtualMachineIsolation2014}, however it typically has the highest overhead, since each function runs in its own virtual machine, with its own kernel. This overhead usually manifests in higher cold start times, however there has been considerable work carried out to reduce this overhead\cite{razaviPrebakedUVMsScalable2015,agacheFirecrackerLightweightVirtualization2020,dawXanaduMitigatingCascading2020,oliverstenbomRefunctionEliminatingServerless2019}.

Kata Containers\cite{KataContainersOpen} provides a container-like interface to VMs, interfacing with existing hypervisors such as Firecracker\cite{agacheFirecrackerLightweightVirtualization2020}, QEMU\cite{QEMU} and Cloud Hypervisor\cite{CloudhypervisorCloudhypervisorVirtual} (previously NEMU\cite{IntelNemu2024}).

Firecracker \cite{agacheFirecrackerLightweightVirtualization2020} is a lightweight KVM\cite{KVM} based Virtual Machine Monitor (VMM) allowing MicroVM provisioning in the order of \qtyrange{125}{150}{\ms} that provides a secure and fast environment for running \faas{} functions.

MicroVMs are intended to be lightweight, and provide a secure environment for running functions, with a fast boot time. Firecracker is written in Rust and unlike QEMU, provides secure, fast and stripped back devices to the guest VM in order to both reduce the attack surface and improve performance\cite{jainStudyFirecrackerMicroVM2020}.

LightVM redesigns the Xen Hypervisor's control plane such that VMs can boot in the order of \qtyrange{2}{3}{\ms}\cite{mancoMyVMLighter2017}, and achieveing a much higher VM density on a host.

\subsection{Container based isolation}
In contrast to VM based isolation, container based isolation is typically considered to be less secure\cite{DemystifyingContainerVs}, however it has a much lower overhead, since each function runs in its own container sharing a kernel with the host\cite{WhatContainerDocker}. From a security perspective, any kernel vulnerabilities could potentially be exploited by a malicious container to escape the sandbox and gain access to the host\cite{linMeasurementStudyLinux2018}.

GVisor is a container runtime that aims to improve security for running containers in a multitenant environment\cite{GVisor}. It implements a userspace `application kernel' that intercepts system calls made by untrusted container, providing a layer of abstraction between any possible vulnerabilities in the host kernel and the code executing in the untrusted container.

Whilst cold-start times of container-based \faas{} isolation are considerably lower than VM based isolation methods, cold-starts times can vary based on a variety of factors. SOCK reduces cold-start times by caching common dependencies in container images, reducing the image footprint and improving cold start times\cite{oakesSOCKRapidTask2018}. Refunction reuses containers between function invocations, reducing cold start times\cite{oliverstenbomRefunctionEliminatingServerless2019}. Pegurus also reuses warm containers between different function invocations to reduce cold start times\cite{liPagurusEliminatingCold2021}. Xanadu reduces cold start times by pre-warming containers based on a predictive model that estimates which subsequent functions will be triggered in a \faas{} workflow\cite{dawXanaduMitigatingCascading2020}.

Apache OpenWhisk\cite{apacheOpenWhisk2024} is an open-source \faas{} platform that provides container based isolation, leveraging Nginx as an HTTP gateway, Kafka as a message broker to queue invocations, CouchDB as a persistent data storage layer and OCI containers to execute arbitrary function logic.

OpenFaaS\cite{ellisOpenFaaS2024} provides container based isolation for \faas{} using Kubernetes to handle scaling and execution of functions, and Prometheus to handle scaling of the functions.

\subsection{V8 isolate based isolation}
Whilst containers provide a much lighter footprint in comparison to VM based isolation, CloudFlare Workers\cite{CloudComputingContainers2018} and Vercel Edge Functions\cite{EdgeRuntime} both utilise V8 isolates to provide lightweight user-space isolation between functions. Whilst less secure than process-based isolation employed by contianers, V8 isolates provide a much denser packing of functions on a single host, and can provide much lower cold start times.

Additional work surrounding microarchitectural vulnerabilities such as Spectre within V8 isolates has been carried out to ensure that V8 isolates can execute securely in multitenant environment\cite{schwarzlRobustScalableProcess2022}.

\subsection{GraalOS and GraalVM based isolation}
\todo[inline]{Discuss how GraalOS and GraalVM reduces cold start times.}

\subsection{Wasm based isolation}
WASM provides a secure and efficient environment for executing untrusted code\cite{WebAssembly} in a multitenant environment. Initially developed to run in the browser, WASM allows for the execution of untrusted code in a sandboxed environment without the need for a full VM or container. It has formal semantics\cite{haasBringingWebSpeed2017}, its embeddings can be formally proven to be memory-safe\cite{SecurefoundationsVWasm2024}, and utilises software based fault isolation to ensure that code cannot escape the sandbox\cite{SecurityWebAssembly}. Despite this effort, microarchitectural vulnerabilities such as Spectre still exist within WASM runtimes, and additional work is being carried out to mitigate these\cite{narayanSwivelHardeningWebAssembly2021}.

Fastly Edge Compute Platform\cite{EdgeCloudPlatform} utilises Lucet\cite{BytecodeallianceLucet2024} (now Wasmtime\cite{Wasmtime}) to sandbox WebAssembly executables from one-another. Fastly Edge Compute Platform cites startup times in the order of tens of microseconds to instantiate the sandbox. The Wasmtime runtime now used by Fastly Edge Compute Platform employs a set of security features to ensure that code executing within the runtime cannot escape the sandbox, and that the runtime itself is secure\cite{SecurityWasmtime}. The guarantees provides by WASM are defined in Section \ref{sec:wasm-safety-mechanisms}. Wasmtime prepends a guard region before linear memories in order to protect against sign-extension bugs in the Cranelift JIT compiler resulting in invalid memory accesses between instances. It utilises guard pages on all thread stacks, triggering an exception if they are hit to prevent data leakage. Additionally, since Wasmtime is written in Rust, the surface which a vulnerability could manifest is reduced to only the unsafe sections of the codebase. The runtime is also implementing CFI mechanisms to use ARM specific instructions

\todo[inline]{WASM safety mechanisms need to be fleshed out}
\section{WASM Safety Mechanisms}
\label{sec:wasm-safety-mechanisms}

In addition to JavaScript functions deployed using V8 isolates, Cloudflare Workers\cite{CloudComputingContainers2018} also allow WASM based functions to be deployed to their edge network. Unlike Fastly, Cloudflare Workers do not execute the function within a WASM runtime, and instead rely on the V8 runtime to execute WASM functions\cite{WebAssemblyWasmCloudflare2024}.

Sledge reduces cold-start times and increases throughput over other WASM based \faas{} frameworks by leveraging LLVM to compile WASM binaries, specifically targetting edge hardware and implements user-space scheduling of functions\cite{gadepalliSledgeServerlessfirstLightweight2020}.

FaaSM employs a similar approach to container based isolation, using CGroups and Namespaces for isolation, and executing WASM binaries. In order to improve interprocess communication and persisting state, FaaSM utilises shared memory to allow functions to communicate with one another\cite{shillakerFaasmLightweightIsolation2020}.

\section{\faas{} billing models}
\label{sec:faas-billing-models}

The \faas{} billing model is a complex model built around a number of factors that vary between cloud providers. Despite their intricacies and differences, the prevailing common billing model, referred to in Equation \ref{eq:faas-billing-model} can be characterised by a function of: a flat-rate invocation cost ($C_i$), and a rate ($C_r$) per unit of time ($G$) charged over the total invocation time ($t$), from the start of a function to the point at which it returns a response, with a minimum billable cost $C_{min}$.

\begin{equation} \label{eq:faas-billing-model}
C_t = C_i + \max\left(C_{min},\ceil*{\frac{t}{G}} C_r\right)
\end{equation}

All the major cloud providers scale their charged rate per unit of time linearly with resource allocation.

Billing cost curves for function resource allocations of \SI{128}{\mega\byte}, \SI{256}{\mega\byte} and \SI{512}{\mega\byte} respectively on each of the three major cloud platforms are shown in Figure \ref{fig:faas-billing-cost-curves}. Notice that the cost curves for AWS Lambda are linear since they have no minimum billable cost, whereas Azure Functions and Google Cloud Functions have a minimum billable cost, resulting in a flat initial cost for the first \SI{100}{\milli\second}.

\begin{figure*}[htp]
    \centering
    \subfigure[AWS Lambda billing cost curve]{
        \centering
        \input{node_modules/@faaas/pricing/assets/billed-cost-aws.pgf}
    }\quad
    \subfigure[Azure Functions billing cost curve]{
        \centering
        \input{node_modules/@faaas/pricing/assets/billed-cost-azure.pgf}
    }\quad
    \subfigure[Google Cloud Functions billing cost curve]{
        \centering
        \input{node_modules/@faaas/pricing/assets/billed-cost-gcf.pgf}
    }
    \caption{\faas{} billing cost curves by cloud provider}
    \label{fig:faas-billing-cost-curves}
\end{figure*}

\subsection{AWS Lambda billing model}
AWS Lambda varies its pricing depending on the datacentre location. As of writing, the pricing model for AWS Lambda is fixed across all regions in the United States, as well as many locations in the EU\cite{ServerlessComputingAWS}.

The flat-rate invocation cost ($C_i$) is uniform regardless of the resources provisioned, and decreases as the number of invocations scales to the millions. The rate per unit of time ($C_r$) is charged based on memory allocation to the function, since function CPU share is not configurable.

Additionally, as of 2020, AWS Lambda changed its billing granularity from \SI{100}{\milli\second} to \SI{1}{\milli\second}\cite{AWSLambdaChanges}, with no minimum billable cost $C_{min}$. As a result, this makes AWS Lambda the only viable option for function splitting with of OLTP workloads, as discussed in Section \ref{sec:double-billing-problem}.

\todo[inline]{Link back to where this change is relevant because its the only provider which has this level of granularity.}

\subsection{Google Cloud Platform}
Google Cloud Functions' (GCF) billing model is similar to AWS Lambda, with a fixed cost per invocation, and a cost per unit of time. GCF charge per \SI{100}{\milli\second} of execution time, rounded up, with a minimum of \SI{100}{\milli\second} per invocation\cite{PricingCloudFunctions}. Unlike AWS Lambda, GCF additionally charged for network traffic outside of the region, and thus for HTTP requests, the size of the response payload must also be accounted for. Also unlike AWS Lambda, GCF allow for different configurations of CPU and memory, which are charged independently ($C_r = C_c + C_m$) of one another.

\subsection{Azure Functions}
Azure Functions use the same billing model, charging per invocation and per unit of execution time, with a granularity of \SI{1}{\milli\second}\cite{PricingFunctionsMicrosoft}. Notably however, they have bill for a minimum execution time of \SI{100}{\milli\second}, with the lowest resource allocation \SI{128}{\mega\byte}.

\section{\caas{} billing models}
In contrast to \faas{} billing models, typically \caasxlong{} billing models offer a coarser blling granularity. AWS, Google Cloud and Azure all offer \caas{} services, with AWS Fargate, Google Cloud Run and Azure Container Instances respectively, to complement their \faas{} offerings.

\subsection{Google Cloud Run}
\label{sec:cloud-run-billing-model}
Google Cloud Run offers a scale-to-zero solution, and the billing only for this configuration will be discussed. Cloud Run bills based on the resource allocation of the container, with a minimum charge of \SI{100}{\milli\second} per invocation\cite{PricingCloudRun}. The billing granularity, similar to Google Cloud Functions, is charged per \SI{100}{\milli\second} of execution time, rounded up, with a minimum of \SI{100}{\milli\second} per invocation. Additionally, Google Cloud Run charges for network egress, and a flat invocation fee per request made to the service.

The core difference between Google Cloud Functions and Google Cloud Run is that the latter charges for the time the container is executing, rather than the time each request takes to resolve. This allows concurrent requests awaiting IO to not be double charged, as described in Section \ref{sec:double-billing-problem}.

\section{V8 and the Event Loop}
\label{sec:js-event-loop}
At the core of every JavaScript runtime is an I/O event loop, whether it be LibUV\cite{LibuvCrossplatformAsynchronous, DesignOverviewLibuv} in V8\cite{googleWhatV82024} based runtimes such as Node.JS\cite{foundationNodeJS2024}, or Tokio\cite{TokioAsynchronousRust} as used in Deno\cite{incDeno2024}.

The I/O event loop is responsible for handling asynchronous IO operations, file system operations, network requests, and IPC in a non-blocking manner. Typically, event loops use operating system level syscalls such as \verb|epoll| and \verb|io_uring|, which are described in more detail in Section \ref{sec:os-level-io}.

\section{Linux asynchronous IO}
\label{sec:os-level-io}

\subsection{What is epoll?}
\todo[inline]{Explain what is the epoll syscall}

\subsection{What is iouring?}
\todo[inline]{Explain what is the iouring syscall}

\section{Double billing problem}
\label{sec:double-billing-problem}

The double billing problem emerges from the fact that \faas{} platforms charge for the time a function is running, and not the time a function is actively executing code. This means that if a function is waiting for a response from an asynchronous service, it is still being billed for the time it spends waiting.

\subsection{Double billing when accessing persistent storage}
Figure \ref{fig:double-billing-db} illustrates this exact scenario whereby a function makes a request to a database, and is billed whilst idle and awaiting the response.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{node_modules/@faaas/double-billing-problem/assets/double-billed-db.pdf}
    \caption{Double billing in action with call to persistent storage. Note that the function handler is idle whilst the DB is executing its query, yet still billed for the entire duration of the function.}
    \label{fig:double-billing-db}
\end{figure}

This is particularly problematic for serverless functions, which are designed to be stateless, and typically communicate with persistent storage to mainain state. This is further compounded by each function instance handling a single request at time.

In other systems with a less finegrained billing model such as \caas{} offerings, an example of which is Google Cloud Run described in Section \ref{sec:cloud-run-billing-model}, where a single instance handles multiple requests, this is not an issue, as the function can perform other useful work, such as accepting additional requests, while waiting for a response from an asynchronous service.

\subsection{Double billing when nesting function invocations}
This issue also manifests more importantly when function invocations are nested as in Figure \ref{fig:double-billing-nested}. Considered an antipattern for this reason\cite{LambdaFunctionsCalling}, nested function invocations lead to a cascading effect of double billing, where each function invocation waits for the next to complete, and is billed for the entire duration of the called function.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{node_modules/@faaas/double-billing-problem/assets/double-billed-nested.pdf}
    \caption{Double billing in action with nested function invocations. Note that the function handler is idle whilst awaiting the nested invocation, yet still billed for the entire duration of the function.}
    \label{fig:double-billing-nested}
\end{figure}

\begin{figure*}[t]
    \begin{center}
        \input{node_modules/@faaas/pricing/assets/min-yield-time.pgf}
    \end{center}
    \caption{\faas{} billing viability for invoking a new function}
\end{figure*}

\section{Characteristics of \faas{} workloads}
A review of serverless use cases and their characteristics by Eismann et al.\cite{eismannReviewServerlessUse2020} found that the majority of serverless functions have shortlived executions on the order of milliseconds to seconds.

It was found that the most popular languages to write serverless functions in were JavaScript, Python. Additionally, it was noted that the majority of applications consisted of 5 or less distinct cloud functions, indicating that large granularity is preferred for serverless functions.

Finally, it was identified that the overwhelming majority of serverless functions interface with persistent block storage, and databases, accounting for 61\% and 47\% respectively.

This further reinforces the notion that \faas{} workloads are much more prone to the double-billing problem, as described further in Section \ref{sec:double-billing-problem}.

\section{AWS Lambda}
AWS Lambda is by far the most popular\cite{eismannReviewServerlessUse2020,StateServerlessDatadog} \faas{} platform used by developers to deploy serverless applications. Unlike Google Cloud Functions, which utilise gVisor\cite{GVisor},

\subsubsection{System architecture}

The AWS Lambda system architecture is centered around the concept of Firecracker MicroVMs\cite{agacheFirecrackerLightweightVirtualization2020}.

\begin{figure*}[t]
    \includegraphics[width=\linewidth]{node_modules/@faaas/aws-lambda-exec-env/assets/aws-lambda-exec-env.pdf}
    \caption{AWS Lambda Execution Environment}
    \label{fig:aws-lambda-exec-env}
\end{figure*}

\subsection{Concurrent executions}

Each invocation of a lambda function executes independently inside of it's own Firecracker MicroVM, in what is known as a slot.

Each MicroVM provides a slot which can handle a single invocation, however once this invocation completes, the slot can be used by another invocation.

\subsection{Pricing structure}

Billed for execution time from start to finish, since a MicroVM is provisioned the entire time.

\section{Continuations and Continuation Passing Style}
A function continuation is a concept in programming\cite{sussmanSCHEMEInterpreterExtended1975}, reified as a datastructure encapsulating an execution state, and a function pointer that can be called to resume execution. They are used extensively across many languages, for example, they form the underpinnings of Rust's Futures API.
