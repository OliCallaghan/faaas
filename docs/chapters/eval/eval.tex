\chapter{Evaluation}
\label{chap:eval}
\begin{figure}
    \begin{center}
        \input{node_modules/@faaas-bench/faaasc-aws-results/assets/aws-strategy-breakdown-olap.pgf}
    \end{center}
    \caption{Cost breakdown of the \texttt{warehouse-order} OLAP function workload running on AWS Lambda (256MB). Using the adaptive split strategy, a cost reduction of 62\% is observed, whilst for always split, a cost reduction of 75\% is observed.}
\end{figure}

In this chapter, \faaas{} is evaluated in terms of its cost savings and overhead on a suite of \faas{} functions executing OLTP, OLAP and mixed workloads. In order to effectively evaluate cost savings if used today, \faas{} is deployed to AWS, and evaluated against the current AWS billing model, resulting in a cost reduction of 75\% across the OLAP benchmark suite. Following this, \faaas{} is then evaluated against a set of hypothetical billing models, in order to evaluate potential cost savings if changes to the billing model were to be made. Finally, key areas that cloud providers need to address are highlighed in order to enable more effective cost reductions across a wider range of \faas{} workloads. Throughout this chapter, we aim to provide answers to the following set of research questions:

\begin{itemize}
    \item[(q1)] To what extent can code splitting reduce costs when executing serverless functions?
    \item[(q2)] What is the overhead incurred by code splitting?
    \item[(q3)] How well does adaptive split identify optimal splitting decisions?
    \item[(q4)] How do different \faas{} billing models impact the viability of function-splitting with \faaas{}?
\end{itemize}

\section{Benchmarking suites}
In this section we will introduce the programs which \faaas{} was evaluated against. There are a set of existing benchmark suites which exist for serverless functions, however these are typically targetted at evaluating \faas{} platforms themselves and their intrinsic properties, so focus much more closely on microbenchmarks rather than real-world workloads. Of the few suites that do exist that aim to be representative of real-world workloads, none query databases, which identified by \cite{eismannReviewServerlessUse2020}, is the second most common use-case for serverless functions after interacting with cloud storage.

Since \faaas{} specifically tackles the double-billing problem described in Section \ref{sec:double-billing-problem}, it is important to profile a wide variety of workloads that are vulnerable to this problem, alongside a set of other programs where this problem does not exist, such that the overhead of the system can be evaluated.

Therefore, a suite of evaluation programs is defined in Table \ref{table:benchmarking-suite}, that perform specific tasks, representative of real-world workloads. These programs are then executed against deployed \faaas{} functions, and the cost savings and overhead are evaluated on AWS. The evaluation programs are segregated into those which execute predominantly OLAP and OLTP workloads, and those which execute a mix of both.

\begin{table*}
\begin{tabularx}{\linewidth}{|r|c|X|c|}
    \hline
    \textbf{Program} & \textbf{PID} & \textbf{Description} & \textbf{Workload} \\
    \hline
    \verb|warehouse-report| & \verb|wh-o| & Executes TPC-H query 1 to generate a pricing report, then generates HTML report using mustache. & OLAP \\
    \hline
    \verb|warehouse-order| & \verb|wh-r| & Executes TPC-H query 2 to compute the minimum cost supplier for a part, and sends SMS (mocked) to the supplier's mobile number requesting the part.  & OLAP \\
    \hline
    \verb|warehouse-pred| & \verb|wh-p| & Executes TPC-H query 5 to compute the countries with the highest revenue, and performs a set of linear algebra operations on this data to simulate a CPU intensive workload. & OLAP \\
    \hline
    \verb|pets| & \verb|pets| & Executes a simple query to sum the pet ages of all pets with a matching name, and returns this in a response. & OLTP \\
    \hline
    \verb|bank| & \verb|bank| & Executes a simple to make a bank balance from one account to another. Returns whether the bank transfer was successful and the new balance. & OLTP \\
    \hline
    \verb|echoer| & \verb|echo| & Dummy function used as a scaffold to evaluate adaptive switch. Makes a request to an external echo service that resolves after a specified number of milliseconds. & OLTP \\
    \hline
\end{tabularx}
\caption{Suite of benchmarking programs used to evaluate \faaas{}}
\label{table:benchmarking-suite}
\end{table*}

\subsection{OLAP workloads}
In order to benchmark a representative OLAP workload, we define a set of serverless functions within the context of the warehouse described in the TPC-H benchmark. The TPC-H benchmark is a `decision support benchmark' that models a warehouse, and is widely used to evaluate the performance of OLAP databases. Whilst the purpose of this research is not to benchmarking the database itself, this benchmarking set provides a representative context for the types of queries which are executed in the OLAP domain.

\subsection{OLTP workloads}
In order to benchmark a representative OLTP workload, we define a set of serverless functions that execute OLTP queries against a database, and return the results. Specifically, we define a function that executes banking transactions within the context of a bank, and a function that queries the ages of a set of pets.

\section{Measuring cost savings}
In this section, we aim to provide provide an answer to the first research question: \blockquote{To what extent can code splitting reduce costs when executing serverless functions?}. In order to do this, we will examine which types of workloads result in cost reductions, and describe specifically how the values for total function cost of the \faaas{} deployment to AWS was measured.

Each function was deployed with each of the following strategies: \verb|adaptive|, \verb|always-proxy|, \verb|never-proxy|, \verb|use-http|, across the following range of memory sizes: 128MB, 256MB, 512MB, 1024MB. Each function was then subjected to a load test using Artillery\cite{artilleryArtilleryCloudscaleLoad}. For each of the deployed AWS Lambda functions, the total billing time and invocation count was extracted from AWS CloudWatch logs using the query defined in Listing \ref{lst:total-cost-cloudwatch-query}. The cost of execution was then calculated using the parameterised billing model defined in Section \ref{sec:faas-param-cost-model} for each function, with each resource configuration and strategy. The results of these tests are tabulated in Table \ref{table:faaas-cost-savings}.

\begin{listing}
\begin{minted}[obeytabs=true,tabsize=2]{fsharp}
fields @timestamp, @message
    | filter @message like /Billed Duration/
    | parse @message "Billed Duration: * ms" as billedDuration
    | parse @message "Memory Size: * MB" as memorySize
    | filter ispresent(billedDuration)
    | stats count(billedDuration) as totalInvocs, sum(billedDuration) as totalDuration by memorySize
\end{minted}
\caption{AWS CloudWatch query to extract billing data for a \faaas{} function}
\label{lst:total-cost-cloudwatch-query}
\end{listing}

\begin{table}
    \centering
    \begin{tabularx}{\linewidth}{|c|c|Z|}\hline
        \textbf{PID} & \textbf{Memory} & \textbf{Cost Reduction (\%)} \\
        \hline
        \csvreader[column count=4,late after line=\\\hline,late after last line=\\\hline]
        {assets/tmp/faaas-bench-faaasc-artillery-mem-results-split-saving.csv}
        {2=\mem, 3=\costsaving, 4=\progid}
        {\texttt{\progid} & \texttt{\mem} & \costsaving}
    \end{tabularx}
    \caption{Cost reduction of each program in the benchmark suite when performing function splitting on AWS Lambda, using the \texttt{always} splitting strategy.}
    \label{table:faaas-cost-savings}
\end{table}

From the results, it is clear that \faaas{} is much less effective at reducing costs for OLTP workloads such as the \verb|pets| workload in Figure \ref{fig:faaas-strategy-breakdown-pets}, whilst for OLAP workloads, provides a substantial cost reduction. The cost reduction observed is dependent entirely on the response time of asynchronous requests. For higher resource allocations, cost savings are greater, and less of a penalty is observed for asynchronous requests with lower response times (such as OLTP requests), as shown in Figure \ref{fig:faaas-mem-prog-strat-heatmap-splitting-saving}.

\begin{figure}
    \begin{center}
        \input{node_modules/@faaas-bench/faaasc-aws-results/assets/aws-strategy-breakdown.pgf}
    \end{center}
    \caption{Cost breakdown of the \texttt{pets} OLTP function workload running on AWS Lambda (256MB). Using the always split strategy, a cost increase of 75\% is observed, indicating that function splitting for short lived requests is not profitable.}
    \label{fig:faaas-strategy-breakdown-pets}
\end{figure}

\begin{figure*}
    \begin{center}
        \includesvg{node_modules/@faaas-bench/faaasc-artillery-mem-results/assets/program-memory-split-saving.svg}
        %\import{node_modules/@faaas-bench/faaasc-artillery-mem-results/assets/}{program-memory-split-saving.pgf}
    \end{center}
    \caption{Cost breakdown of the \texttt{pets} OLTP function workload running on AWS Lambda (256MB). Using the always split strategy, a cost increase of 75\% is observed, indicating that function splitting for short lived requests is not profitable.}
    \label{fig:faaas-mem-prog-strat-heatmap-splitting-saving}
\end{figure*}

\section{Measuring \faaas{} overhead}
In this section, we aim to provide an answer to the second research question: \blockquote{What is the overhead incurred by code splitting?}. In order to do this, we will evaluate how function response times are affected by introducing various parts of the \faaas{} AWS deployment, and describe specifically how figures for function response times of the \faaas{} deployment to AWS was measured. Additionally, we will provide a breakdown of billed time whilst executing a \faaas{} split function in comparison with executing the original function.

For each of the deployed functions, under the load tests, the average response time, along with other metrics were extracted from the Artillery output. The artillery experiment was executed from a home WiFi network, with a \SI{67.1}{Mbps} download, and \SI{17.1}{Mbps} upload speed. The average, and percentile response times were then calculated for each function, for each resource configuration and splitting strategy.

From the experiments, it is clear that \faaas{} introduces a high latency overhead as can be seen in \ref{table:faaas-response-time-latency}. For the \verb|warehouse-order| benchmark, this value was approximately 8 times higher compared to invoking functions directly through AWS Lambda's HTTP endpoints. By profiling which sections of the architecture consumed the most of this time, it emerged that the majority of the time is spent publishing and consuming messages from the RabbitMQ queue.

\begin{figure}
    \begin{center}
        \input{node_modules/@faaas-bench/faaasc-artillery-results/assets/mean_response_times.pgf}
    \end{center}
    \caption{Artillery response times}
\end{figure}

\begin{table}
    \centering
    \begin{tabularx}{\linewidth}{|c|c|Z|Z|}\hline
        \textbf{PID} & \textbf{Strategy} & \textbf{Mean (ms)} & \textbf{p95 (ms)} \\
        \hline
        \csvreader[column count=13,late after line=\\\hline,late after last line=\\\hline]
        {assets/tmp/faaas-bench-faaasc-artillery-results-response-times.csv}
        {1=\name, 13=\strat, 5=\mean, 10=\p95}
        {\texttt{\name} & \texttt{\strat} & \mean & \p95}
    \end{tabularx}
    \caption{Response time latency impact of using \faaas{}}
    \label{table:faaas-response-time-latency}
\end{table}

In order to provide a breakdown of the time spent during billed function invocation time, the AWS \faaas{} adaptor was instrumented to log times spend executing each part of the function during execution. This data was extracted from CloudWatch logs, and analysed into Table \ref{table:faaas-duration-breakdown}, and visualised for both the pets OLTP function \ref{fig:faaasc-oltp-duration-bill-breakdown}.

\begin{table}
    \centering
    \begin{tabularx}{\linewidth}{|c|c|Z|Z|}\hline
        \textbf{PID} & \textbf{Strategy} & \textbf{Mean} & \textbf{p95} \\
        \hline
        \csvreader[column count=13,late after line=\\\hline,late after last line=\\\hline]
        {assets/tmp/faaas-bench-faaasc-artillery-results-response-times.csv}
        {1=\name, 13=\strat, 5=\mean, 10=\p95}
        {\texttt{\name} & \texttt{\strat} & \mean & \p95}
    \end{tabularx}
    \todo[inline]{Generate this data}
    \caption{Duration breakdown of \faaas{} function execution}
    \label{table:faaas-duration-breakdown}
\end{table}

\begin{figure}
    \begin{center}
        \input{node_modules/@faaas-bench/faaasc-aws-results/assets/aws-strategy-duration-breakdown.pgf}
    \end{center}
    \caption{OLTP \faaasc{} functions running in AWS, billing breakdown specifically of the duration segment of billed.}
    \label{fig:faaasc-oltp-duration-bill-breakdown}
\end{figure}

\section{Adaptive splitting strategy}
In this section, we aim to provide an answer to the third research question: \blockquote{How well does adaptive split identify optimal splitting decisions?} We will measure the cost of executing a variable workload using the \verb|adaptive|, \verb|never| and \verb|always| strategies over time.

The setup is as follows: we deploy the \verb|echoer| function to AWS Lambda, and use Artillery\cite{artilleryArtilleryCloudscaleLoad} to send requests at a rate of 1 request per second for 45mins to each of the strategies. Artillery is configured such that every 15mins, the timeout sent to \verb|echoer| changes from \SI{50}{\milli\second} to \SI{200}{\milli\second}, which should in turn cause the adaptive strategy to adjust it's splitting strategy once the monitor runs.

The total cost of each strategy is plotted over time in Figure \ref{fig:strategy-cost-over-time}. Whilst the adaptive strategy does not instantly adjust its strategy, after a short period of time, around 10 minutes of sustained high latency, it updates its strategy selects the optimal strategy. This value can be tuned, however since with AWS it relies on querying CloudWatch data, it is somewhat limited by the propagation delay of CloudWatch logs (typically 1-3mins). Additionally, the monitor analyses historical data in a sliding \SI{10}{\min} sliding window in order to smooth any spikes in data.

\begin{figure*}
    \begin{center}
        \input{node_modules/@faaas-bench/faaasc-adaptive-results/assets/split-profitability.pgf}
    \end{center}
    \caption{Strategy cost over time.}
    \label{fig:strategy-cost-over-time}
\end{figure*}

In figure \ref{fig:strategy-decision-over-time}, the latency inferred by the monitor and actual obvserved latency is overlayed ontop of the computed probability of profitability by the function when selecting which strategy to use. During the first \SI{10}{\min}, the monitor is fitting the response time distribution to both low and high latency echoes, with the proportion of high latency echoes increasing throughout this period. As a result, it is clear from the plot that the probability of profitability is increasing during this period. There is however a brief period at 10:25, where the inferred latency diverges to a high negative value. This causes the algorithm to switch back to the never strategy briefly, before switching back to the adaptive strategy once the distribution converges back to the actual latency.

\begin{figure*}
    \begin{center}
        \input{node_modules/@faaas-bench/faaasc-adaptive-results/assets/decision-mapping.pgf}
    \end{center}
    \caption{Strategy cost over time.}
    \label{fig:strategy-decision-over-time}
\end{figure*}

\section{Analysis of billing model dynamics on \faaas{} viability}
In this final section, we will discuss how each of the above findings help to provide an answer to the final research question: \blockquote{How do different \faas{} billing models impact the viability of function-splitting with \faaas{}?}.

From the results of the experiments on AWS, whilst we observe a significant cost reduction in workloads executing OLAP queries, those executing predominantly OLTP queries are unable to observe any cost reductions, and actually incur penalties by performing code splitting. This is due to the high invocation costs relative to the duration costs billed when executing the function.

Since AWS is not the only provider of \faas{}, it is important to consider how this evaluation could be different if the pricing model changed in the future, or a different provider emerged with a different billing structure.

\subsection{Changes to invocation cost pricing}
Invocation costs for cloud functions are loosely coupled to the technical overhead experienced when executing a \faas{} function. Therefore, by reducing the overhead of invoking a function, it would be expected that this cost would also decrease.

Referring back to the cost model in Section \ref{sec:faas-param-cost-model}, by reducing the invocation cost, it would reduce the critical threshold which it becomes profitably to split a function across an asynchronous request.

\subsection{Reduction in startup times (cold-start and warm-start)}
From the overhead analysis, it is shown that there is considerable overhead when reinvoking a function above typical function cold starts. This is is due to a variety of factors, including the extra network round-trips to and from RabbitMQ, in addition to the additional setup costs of invoking a function with Lambda.

Since this overhead has constant time, its fiscal penalty is proportional to the function resource allocation. Similarly to the flat invocation cost, the overhead penalty occupies a larger proportion of the duration cost when the duration time is short, and thus this impacts workloads that execute OLTP queries.

Reducing this overhead would make it more cost-effective to split functions across OLTP queries as the overhead would be less significant relative to the duration cost.

\subsection{Changes to duration cost pricing}
Conversely, an increase in the duration cost $C_r$ of the cloud provider platform would make it more cost-effective to split functions across OLTP queries. The \faaasc{} AWS Lambda adaptor would be able to support batching invocations with a small number of modifications, so by increasing the resource allocation, a Lambda could process more requests in the same invocation.

This would have the impact that the invocation cost of a single is spread evenly across all the requests in the batch. This would make it more cost-effective to split functions across OLTP queries, as the invocation cost currently dominates. On the contrary however, this model requires invocations the same underlying handler to be batched, and typically, OLTP queries are not batched.
